#!/bin/bash
#SBATCH --job-name=first_test
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=2
#SBATCH --gpus-per-task=1
#SBATCH --mem=16G
#SBATCH --partition=gpu_course
#SBATCH --time=00:10:00
#SBATCH --output=output_%j.out
#SBATCH --error=error_%j.err
echo "Running on node: $(hostname)"
echo " Job ID: $SLURM_JOB_ID"
module load 2025

import torch
import time

print("First Batch Job - assignment 1")
print(f"PyTorch v: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
print(f"GPU Device: {torch.cuda.get_device_name(0)}")

x = torch.randn(10, 10)
y = torch.randn(10, 10)
start = time.time()
z = torch.matmul(x, y)
elapsed = time.time() - start
print(f"Matrix multiplication (10x10) took: {elapsed:.4f} seconds")
